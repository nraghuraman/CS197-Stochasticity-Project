# CS197-Stochasticity-Project
This was a research project I completed with Eric Lou and Leo Jean-Baptiste of Stanford University for Stanford's computer science research course. This project tests a technique called "A Barrage of Random Transforms" (BaRT), which is intended to make neural networks robust against adversarial inputs. Specifically, we tested BaRT against forms of unforeseen adversarial inputs that the original BaRT developers did not utilize. My team members and I each contributed to a thorough review of the existing literature, developed, trained, and tested our neural network, and presented our findings in a research paper and presentation.

We ultimately determined that BaRT was ineffective at responding to unforeseen adversarial inputs. We are interested in publishing our research, and we encourage others to verify these findings when we do so. Here are a few ideas that we were unable to integrate into our research but we would encourage future researchers to consider:
- BaRT works by applying random transformations to images before they are inputted into the neural network. While we determined that BaRT as a whole was ineffective, we were not sure whether some transformations are more or less effective than others. We would encourage future researchers to explore this.
- We always randomly applied between 0 and 5 transformations to images. However, we would be interested to see if there is another range (e.g. 2-10 transformations) that is more optimal.

This repository contains code for the training and testing of our neural networks against adversarial examples. To see the code we utilized to generate adversarial examples, visit https://github.com/ezl-13/advex-uar, which is a fork of the original Advex-UAR code developed by Daniel Kang, et al. for the research paper "Testing Robustness Against Unforeseen Adversaries."
